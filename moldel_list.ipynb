{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/j0gea/AI-data-competition-Wisdomtooth/blob/main/moldel_list.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krz2-Jtqm8Vb"
      },
      "source": [
        "# 모델"
      ],
      "id": "krz2-Jtqm8Vb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "b8ce8f6d",
        "outputId": "9a022230-11c2-4d0c-b421-6c06cdd84a61"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-dde8b4ee8073>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtimm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'timm'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import timm"
      ],
      "id": "b8ce8f6d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a4f5d68"
      },
      "source": [
        "#### 파이토치\n",
        "- torchvision.models.모델명\n",
        "\n",
        "#### timm\n",
        "- timm.create_model('모델명')\n",
        "\n",
        "- 깔린 모델들 /USER/models.py\n",
        "- 모델들 샘플 폴더 /USER/model_folder"
      ],
      "id": "5a4f5d68"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b318ac06"
      },
      "source": [
        "### (0) 기본 샘플 코드\n",
        "\n",
        "- 모델: densenet121(아래 샘플. 하지만 혼용됩니다(아마?))\n"
      ],
      "id": "b318ac06"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6f64358b"
      },
      "outputs": [],
      "source": [
        "class BaselineModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BaselineModel, self).__init__()\n",
        "\n",
        "        self.model = torchvision.models.densenet121(pretrained=True)\n",
        "        n_features = self.model.classifier.in_features\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(n_features, 512),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, 1)\n",
        "        )\n",
        "        self.model.classifier = self.fc\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        return x"
      ],
      "id": "6f64358b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "d3dbb135"
      },
      "outputs": [],
      "source": [
        "# Fully Connected Layer 구조\n",
        "densenet121 = torchvision.models.densenet121(pretrained=True)\n",
        "densenet121.classifier"
      ],
      "id": "d3dbb135"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb09e664"
      },
      "source": [
        "### (1) convnextv2 (224)\n",
        "\n",
        "- convnextv2_large\n",
        "- convnextv2_base\n",
        "- convnextv2_tiny (아래 코드)\n",
        "\n",
        "가중치 적용 (fcmae, ft, in22k, in1k)\n",
        "- convnextv2_large.fcmae_ft_in22k_in1k\n",
        "- convnextv2_base.fcmae_ft_in22k_in1k\n",
        "- convnextv2_tiny.fcmae_ft_in22k_in1k"
      ],
      "id": "bb09e664"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "c57cb514"
      },
      "outputs": [],
      "source": [
        "class BaselineModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BaselineModel, self).__init__()\n",
        "\n",
        "        self.model = timm.create_model('convnextv2_tiny', pretrained=True)\n",
        "        n_features = self.model.head.fc.in_features\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(n_features, 512),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, 1)\n",
        "        )\n",
        "        self.model.head.fc = self.fc\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        return x"
      ],
      "id": "c57cb514"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "81450abb"
      },
      "outputs": [],
      "source": [
        "# Fully Connected Layer 구조\n",
        "convnextv2 = timm.create_model('convnextv2_tiny', pretrained=True)\n",
        "convnextv2.head.fc"
      ],
      "id": "81450abb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01366ec9"
      },
      "source": [
        "### (2) SE-ResNeXt\n",
        "\n",
        "- seresnext50_32x4d (224 * 224)\n",
        "- seresnextaa101d_32x8d\n",
        "\n",
        "- seresnext50_32x4d.racm_in1k\n",
        "- seresnextaa101d_32x8d.sw_in12k_ft_in1k_288 (320 or 228)"
      ],
      "id": "01366ec9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9a5d207a"
      },
      "outputs": [],
      "source": [
        "class BaselineModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BaselineModel, self).__init__()\n",
        "\n",
        "        self.model = timm.create_model('seresnext50_32x4d', pretrained=True)\n",
        "        n_features = self.model.fc.in_features\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(n_features, 512),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, 1)\n",
        "        )\n",
        "        self.model.fc = self.fc\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        return x"
      ],
      "id": "9a5d207a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5a14aa4d"
      },
      "outputs": [],
      "source": [
        "model = timm.create_model('seresnext50_32x4d', pretrained=True)\n",
        "model.fc"
      ],
      "id": "5a14aa4d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5ecd9ce"
      },
      "source": [
        "### (3) EfficientNet\n",
        "\n",
        "- B0 224\n",
        "- B1 240\n",
        "- B2 260\n",
        "- B3 300\n",
        "- B4 380\n",
        "- B5 456\n",
        "- B6 528\n",
        "- B7 600"
      ],
      "id": "d5ecd9ce"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6b2a35c1"
      },
      "outputs": [],
      "source": [
        "from efficientnet_pytorch import EfficientNet"
      ],
      "id": "6b2a35c1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cd8c521d"
      },
      "outputs": [],
      "source": [
        "class BaselineModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BaselineModel, self).__init__()\n",
        "\n",
        "        self.model = EfficientNet.from_pretrained('efficientnet-b4')\n",
        "        n_features = self.model._fc.in_features\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(n_features, 512),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, 1)\n",
        "        )\n",
        "        self.model._fc = self.fc\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        return x"
      ],
      "id": "cd8c521d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "34a2110c"
      },
      "outputs": [],
      "source": [
        "EfficientNet = EfficientNet.from_pretrained('efficientnet-b4')\n",
        "EfficientNet._fc"
      ],
      "id": "34a2110c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "v5izi5IWm_D7"
      },
      "outputs": [],
      "source": [],
      "id": "v5izi5IWm_D7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pCCyxlRMZ8z"
      },
      "source": [
        "### (4)ViT"
      ],
      "id": "1pCCyxlRMZ8z"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "oyB5GkZXNaDj"
      },
      "outputs": [],
      "source": [
        "# 이진 분류여서 num_classes=1로 설정(이것도 추가한 부분)"
      ],
      "id": "oyB5GkZXNaDj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Ajrit8QiMgHa"
      },
      "outputs": [],
      "source": [
        "import timm\n",
        "from torch.optim import lr_scheduler"
      ],
      "id": "Ajrit8QiMgHa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OPORKdKXMq0m"
      },
      "outputs": [],
      "source": [
        "class BaselineModel(nn.Module):\n",
        "    def __init__(self,num_classes=1):\n",
        "        super(BaselineModel, self).__init__()\n",
        "        self.model = timm.create_model('vit_base_patch16_224',pretrained=True, num_classes=1)\n",
        "        # n_features = self.model.classifier.in_features\n",
        "        # self.fc = nn.Sequential(\n",
        "        #     nn.Linear(n_features, 512),\n",
        "        #     nn.LeakyReLU(),\n",
        "        #    nn.Dropout(0.5),\n",
        "        #    nn.Linear(512, 1)\n",
        "        #)\n",
        "        #self.model.classifier = self.fc\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        return x"
      ],
      "id": "OPORKdKXMq0m"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6yRo8pQSM-Xp"
      },
      "outputs": [],
      "source": [
        "# 모델 초기화 및 criterion, optimizer 설정 부분에 exp_lr_schedular 추\n",
        "# Initialize the model, loss function, and optimizer\n",
        "#model = BaselineModel().to(device)\n",
        "#criterion = nn.BCEWithLogitsLoss()\n",
        "#optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n"
      ],
      "id": "6yRo8pQSM-Xp"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZ952eNaOvOe"
      },
      "source": [
        "---"
      ],
      "id": "pZ952eNaOvOe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "IyQfsI0eWg_C"
      },
      "outputs": [],
      "source": [
        "class BaselineModel(nn.Module):\n",
        "    def __init__(self,num_classes=1):\n",
        "        super(BaselineModel, self).__init__()\n",
        "        self.model = timm.create_model('vit_base_patch16_224',pretrained=True, num_classes=1)\n",
        "        n_features = self.model.head.in_features\n",
        "        self.model.fc_norm = nn.Sequential(nn.Linear(n_features, 512), nn.LeakyReLU())\n",
        "        self.model.head_drop = nn.Dropout(0.5)\n",
        "        self.model.head = nn.Linear(512, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        return x"
      ],
      "id": "IyQfsI0eWg_C"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FBjm0LHm_lE"
      },
      "source": [
        "# 기타 코드"
      ],
      "id": "1FBjm0LHm_lE"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBCO15L_nQbj"
      },
      "source": [
        "### early stpping code"
      ],
      "id": "OBCO15L_nQbj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Y9WZ735nBdm"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "\n",
        "# early stopping code\n",
        "early_stopping_epochs = 5\n",
        "best_loss = float('inf')\n",
        "early_stop_counter = 0\n",
        "\n",
        "train_loss_list, val_loss_list, val_metric_list = [], [], []\n",
        "for epoch in range(num_epochs):\n",
        "    train_losses = train(model, train_loader, criterion, optimizer, device)\n",
        "    val_losses, val_metrics = valid(model, val_loader, criterion, device)\n",
        "    print('Epoch {}, Train Loss: {:.4f}'.format(epoch+1, np.mean(train_losses)))\n",
        "    print('Epoch {}, Train Loss: {:.4f}, Valid Loss: {:.4f}, Valid Metric: {:.4f}'.format(epoch+1, np.mean(train_losses), np.mean(val_losses), np.mean(val_metrics)))\n",
        "\n",
        "    # Check for early stopping start\n",
        "    mean_val_loss = np.mean(val_losses)\n",
        "    val_metric = np.mean(val_metrics)\n",
        "\n",
        "    if mean_val_loss < best_loss:\n",
        "        best_loss = mean_val_loss\n",
        "        early_stop_counter = 0\n",
        "        # Save model\n",
        "        torch.save(model.state_dict(), './model.pth')\n",
        "    else:\n",
        "        early_stop_counter += 1\n",
        "\n",
        "    if early_stop_counter >= early_stopping_epochs:\n",
        "        print(f'Early stopping at epoch {epoch+1}')\n",
        "        break\n",
        "    # Check for early stopping end\n"
      ],
      "id": "8Y9WZ735nBdm"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cg8F_sYrb5qe"
      },
      "source": [
        "### optuna\n",
        "\n",
        "코드 위치: /USER/YJtest/sample.py\n",
        "\n",
        "코드 위치 :/USER/YJtest/Convnext_baseline/train.py"
      ],
      "id": "cg8F_sYrb5qe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iogbYVfHb8Qo"
      },
      "outputs": [],
      "source": [
        "import optuna"
      ],
      "id": "iogbYVfHb8Qo"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6IHkaty4b9_4"
      },
      "outputs": [],
      "source": [
        "def objective(trial):\n",
        "    num_epochs = trial.suggest_int('num_epochs', 5, 50)\n",
        "    num_batches = trial.suggest_int('num_batches', 16, 128)\n",
        "\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomRotation(30),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    val_transform = transforms.Compose([\n",
        "        transforms.Resize(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "\n",
        "    for train_idx, val_idx in skf.split(df, df['risk']):\n",
        "        train_df = df.iloc[train_idx]\n",
        "        val_df = df.iloc[val_idx]\n",
        "        break\n",
        "\n",
        "    train_dataset = BaselineDataset(train_df, transform=train_transform)\n",
        "    val_dataset = BaselineDataset(val_df, transform=val_transform)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=num_batches, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=num_batches, shuffle=False)\n",
        "    model = BaselineModel().to(device)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        losses = []\n",
        "        for inputs, labels in tqdm(train_loader):\n",
        "            inputs, labels = inputs.to(device), labels.float().to(device)\n",
        "            #inputs, labels = inputs.to(device), labels.float().to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs).view(-1)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            losses.append(loss.item())\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {np.mean(losses)}')\n",
        "\n",
        "    return np.mean(losses)\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=2)\n",
        "\n",
        "best_trial = study.best_trial\n",
        "print(f\"Best trial - Loss: {best_trial.value}, Params: {best_trial.params}\")"
      ],
      "id": "6IHkaty4b9_4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMkvqB9aOoup"
      },
      "source": [
        "### F1 예측 코드\n",
        "- 조금 더 표본이 필요함\n",
        "- 당연하지만 정확하지 않음\n",
        "- 사용할때 예측코드 복사해서 사용하는걸 추천\n",
        "\n",
        "- output: **F1test**_submission.csv 인데 **절대!! 실수로 제출하지 않도록!!**"
      ],
      "id": "NMkvqB9aOoup"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWo-9s9hQ3Bw"
      },
      "source": [
        "- True Positive (TP) : “high”으로 정확하게 예측된 샘플의 수 (예측 h 실제 h)\n",
        "- False Positive (FP) : “high”으로 잘못 예측된 샘플의 수 (예측 h 실제 l\n",
        "- True Negatives (TN) : “low”으로 올바르게 예측된 샘플의 수 (예측 l 실제 l)\n",
        "- False Negatives (FN) : “low”으로 잘못 예측된 샘플의 수 (예측l 실제h)\n",
        "\n",
        "precision = high로 정확하게 예측된 샘플의 수 / 전체 high로 예측 한 수\n",
        "TP / TP+FP\n",
        "\n",
        "recall = high로 정확하게 예측된 샘플의 수 / 실제 high의 수\n",
        "TP / TP+FN\n",
        "\n",
        " 2 * ((precision * recall) / (precision + recall))\n",
        "\n",
        " 각 레이블 별로 계산, 평균내기(low high 계산)"
      ],
      "id": "zWo-9s9hQ3Bw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5azElLROvRX"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "preds_list = []\n",
        "with torch.no_grad():\n",
        "    for image in tqdm(test_loader):\n",
        "        image = image.to(device)\n",
        "        outputs = model(image).view(-1)\n",
        "\n",
        "        preds = torch.sigmoid(outputs).round()\n",
        "        preds_list += preds.cpu().numpy().tolist()\n",
        "\n",
        "# start here\n",
        "test_df['pre_risk'] = preds_list\n",
        "test_df['pre_risk'] = test_df['pre_risk'].apply(lambda x: 'high' if x == 1 else 'low')\n",
        "test_df['result'] = test_df['risk'] == test_df['pre_risk']\n",
        "\n",
        "#high f1\n",
        "high = test_df.loc[test_df.pre_risk == 'high'] # TP+FP\n",
        "Rhigh = test_df.loc[test_df.risk == 'high'] # TP+FN\n",
        "\n",
        "TP = high.loc[high.risk == 'high'] # TP\n",
        "\n",
        "precision = len(TP) / len(high)\n",
        "recall = len(TP) / len(Rhigh)\n",
        "F1_high = 2 * ((precision * recall) / (precision + recall))\n",
        "\n",
        "#print(F1_high)\n",
        "\n",
        "#low f1\n",
        "low = test_df.loc[test_df.pre_risk == 'low'] # TP+FP\n",
        "Rlow = test_df.loc[test_df.risk == 'low'] # TP+FN\n",
        "\n",
        "TP = low.loc[low.risk == 'low'] # TP\n",
        "\n",
        "precision = len(TP) / len(low)\n",
        "recall = len(TP) / len(Rlow)\n",
        "F1_low = 2 * ((precision * recall) / (precision + recall))\n",
        "\n",
        "#print(F1_low)\n",
        "\n",
        "F1 = (F1_high + F1_low) / 2\n",
        "\n",
        "print(\"F1 test score : \", F1)\n",
        "\n",
        "test_df.to_csv('./F1test_submission.csv', index=False)\n",
        "# End here"
      ],
      "id": "q5azElLROvRX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7tT5dVcO23-"
      },
      "source": [
        "아래 두 위치를 train으로 바꾸어주어야 합니다\n",
        "그거 기반으로 하는거라"
      ],
      "id": "Q7tT5dVcO23-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tkjW1PgXO0VE"
      },
      "outputs": [],
      "source": [
        "img_fname = f'/DATA/train/images/{img_name}'\n",
        "test_df = pd.read_csv(f'/DATA/train/train.csv')"
      ],
      "id": "tkjW1PgXO0VE"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZfB90i_f0s0"
      },
      "source": [
        "### Gradient Accumulation\n",
        "\n",
        "참고 사이트\n",
        "\n",
        "- https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255 (code)\n",
        "\n",
        "- https://gist.github.com/thomwolf/ac7a7da6b1888c2eeac8ac8b9b05d3d3#file-gradient_accumulation-py (위 코드 깃허브)\n",
        "\n",
        "- https://velog.io/@nawnoes/Pytorch%EB%A1%9C-%ED%81%B0-%EB%AA%A8%EB%8D%B8-%ED%95%99%EC%8A%B5%EC%8B%9C-%EC%96%B4%EB%96%BB%EA%B2%8C-%EB%B0%B0%EC%B9%98-%EC%82%AC%EC%9D%B4%EC%A6%88%EB%A5%BC-%EB%8A%98%EB%A6%B4%EC%88%98-%EC%9E%88%EC%9D%84%EA%B9%8C (한국어 이론)"
      ],
      "id": "uZfB90i_f0s0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0L_mNKVkhFHm"
      },
      "outputs": [],
      "source": [
        "# Define the training function\n",
        "# accumulation_steps (How much to slice)\n",
        "def train(model, train_loader, criterion, optimizer, device, accumulation_steps):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    for i, (inputs, labels) in enumerate(tqdm(train_loader)):\n",
        "        inputs, labels = inputs.to(device), labels.float().to(device)\n",
        "\n",
        "        outputs = model(inputs).view(-1)\n",
        "        # 1 d vecter (view(-1))\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss = loss / accumulation_steps\n",
        "        loss.backward()\n",
        "\n",
        "        if (i + 1) % accumulation_steps == 0:\n",
        "            optimizer.step()\n",
        "            model.zero_grad()\n",
        "\n",
        "        losses.append(loss.item())\n",
        "    return losses"
      ],
      "id": "0L_mNKVkhFHm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXtEkzBMh9Eq"
      },
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "# num_batches  = want_num_batches / accumulation_steps\n",
        "num_epochs = 40\n",
        "num_batches = 16 # same as 32\n",
        "\n",
        "# How much to slice\n",
        "accumulation_steps = 2"
      ],
      "id": "gXtEkzBMh9Eq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3fU7y9zOiXhj"
      },
      "outputs": [],
      "source": [
        "train_losses = train(model, train_loader, criterion, optimizer, device, accumulation_steps)\n",
        "# added accumulation_steps"
      ],
      "id": "3fU7y9zOiXhj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qx6o6xkzulYJ"
      },
      "source": [
        "### 결과 txt파일 내놓기"
      ],
      "id": "qx6o6xkzulYJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncfXs92Buk8q"
      },
      "outputs": [],
      "source": [
        "f = open(\"result.txt\",'w')\n",
        "\n",
        "data = 'Epoch {}, Train Loss: {:.4f}, Valid Loss: {:.4f}, Valid Metric: {:.4f}\\n'.format(epoch+1, np.mean(train_losses), np.mean(val_losses), np.mean(val_metrics))\n",
        "f.write(data)\n",
        "\n",
        "f.close()"
      ],
      "id": "ncfXs92Buk8q"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiqWpN476tSC"
      },
      "source": [
        "---"
      ],
      "id": "XiqWpN476tSC"
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "krz2-Jtqm8Vb"
      ],
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}